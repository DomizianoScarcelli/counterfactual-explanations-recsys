import pytest
from aalpy.automata.Dfa import Dfa
from dataset_generator import NumItems
from recommenders.test import model_predict, load_dataset, load_data, generate_model
from recbole.config import Config
from tqdm import tqdm
from automata_learning import (
    run_automata, 
    generate_automata_from_dataset, 
    generate_single_accepting_sequence_dfa, 
)

# Fixtures
@pytest.fixture
def dataset():
    return load_dataset(load_path="saved/counterfactual_dataset.pickle")

@pytest.fixture
def automata(dataset):
    good_points, bad_points = dataset
    return generate_automata_from_dataset((good_points, bad_points))

@pytest.fixture
def automata_gt(dataset):
    good_points, _ = dataset
    return good_points[0][1].item()

@pytest.fixture
def good_point(dataset):
    good_points, _ = dataset
    return good_points[0][0]

# Test functions
def test_automata(automata: Dfa, dataset):
    """
    Test if automata accepts good sequences on the learning set of good points
    and bad points.
    """
    good_points, bad_points = dataset
    for good_point, bad_point in zip(good_points, bad_points):
        good_result = run_automata(automata, good_point[0].tolist())
        bad_result = run_automata(automata, bad_point[0].tolist())
        assert good_result, f"Wrong result for good point: {good_result}"
        assert not bad_result, f"Wrong result for bad point: {bad_result}"

def test_automata_against_bb(automata: Dfa, automata_gt: int):
    """
    Test if automata accepts good sequences (model(x) == gt) on the entire test
    set and refuses bad sequences (model(x) != gt) and calculates the
    precision. The precision refers to how much the automaton can approximate
    the neighbourhood of x described by the black box model
    """
    parameter_dict_ml1m = {
        'load_col': {"inter": ['user_id', 'item_id', 'rating', 'timestamp']},
        'train_neg_sample_args': None,
        "eval_batch_size": 1
    }
    config = Config(model='BERT4Rec', dataset='ml-1m', config_dict=parameter_dict_ml1m)
    train_data, valid_data, test_data = load_data(config)
    model = generate_model(config)
    
    good_predictions, bad_predictions = 0, 0
    same_prediction, diff_prediction = 0,0
    for _, data in enumerate(tqdm(test_data, "Testing automata against black box model...")):
        interaction = data[0]
        point = interaction.interaction["item_id_list"].squeeze(0)
        bb_label = model_predict(point, prob=False, default_interaction=interaction, default_model=model)
        automata_accepts = run_automata(automata, point.tolist())
        same_label = (bb_label == automata_gt and automata_accepts)
        diff_label = (bb_label != automata_gt and not automata_accepts)
        good_prediction = same_label or diff_label

        if good_prediction: good_predictions += 1
        else: bad_predictions += 1

        if same_label: same_prediction += 1
        if diff_label: diff_prediction += 1

    total_predictions = good_predictions + bad_predictions
    precision = (good_predictions / total_predictions) * 100 if total_predictions > 0 else 0
    assert precision > 90, f"Automata precision is too low: {precision}%"
    print(f"Precision: {precision}%")
    print(f"Correct Same label: {same_prediction}/{total_predictions}")
    print(f"Correct Diff label: {diff_prediction}/{total_predictions}")
